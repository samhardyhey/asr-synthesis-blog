{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2fac16f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_colwidth = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e797ea1",
   "metadata": {},
   "source": [
    "## pyttsx3\n",
    "- cannot synthesize a list of strings sequentially? - issue replicated with script, not a notebook issue\n",
    "- cannot supply \"complex\" output dir - eg. nested within otso-rand bucket\n",
    "- can only process list of two strings sequentially\n",
    "- outputed audio files do not play? when generated from script or in sequence\n",
    "- tested a variety of different transcript outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "6a46402a",
   "metadata": {},
   "outputs": [],
   "source": [
    "self_chat = list(srsly.read_jsonl('/Users/samhardyhey/Downloads/TransformerGenerator_1_TransformerGenerator_2_selfchat.jsonl'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "74f28d6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_episode(episode_raw):\n",
    "    # as a dataframe\n",
    "    episode = []\n",
    "    for e in episode_raw['dialog']:\n",
    "        episode.extend([{'id': ee['id'], 'text': ee['text']} for ee in e])\n",
    "\n",
    "    return (pd.DataFrame(episode)\n",
    "              .reset_index()\n",
    "              .rename(mapper={'index': 'exchange_index'}, axis='columns')\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c2edc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\"[TransformerGenerator_1]: hi i am from the city how are you ? i live in california . where are you from ?\n",
    "   [TransformerGenerator_2]: i ' d love to visit cali . i ' ve never been there . i am 56 .\n",
    "[TransformerGenerator_1]: i love california its beautiful ! i ' m in my mid 50 ' s . where do you live ?\n",
    "   [TransformerGenerator_2]: i live on the west coast . i used too be pretty , but i am not anymore . you ?\n",
    "[TransformerGenerator_1]: i live near san francisco . what do you do for fun ? i am an alcoholic but i love it !\n",
    "   [TransformerGenerator_2]: i love to fantasize . i have dreamed of being the world and taking over . do you ?\n",
    "[TransformerGenerator_1]: i do not ! i love the outdoors . i love being in the mountains ! what are you up to ?\n",
    "   [TransformerGenerator_2]: i was a painter , now i work as a housemaid . you ever been to cali ?\n",
    "[TransformerGenerator_1]: i have not , but my family would love to go . i grew on a very small farm .\n",
    "   [TransformerGenerator_2]: that ' s nice . where did you grow the food ? do you have any hobbies ?\"\"\"\n",
    "\n",
    "processed_transcripts = []\n",
    "for e in transcript.split('\\n'):\n",
    "    # remove brackets, and computer generated names\n",
    "    ee = e.strip().replace('[', '').replace(']', '').replace(\n",
    "        'TransformerGenerator', 'Speaker').strip()\n",
    "    processed_transcripts.append(ee)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c807b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially split each utterance into sentences, allow for padding/spacing in audio?\n",
    "import spacy\n",
    "nlp = spacy.load('en_core_web_md')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3220836e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.say(list(nlp(processed_transcripts[1].split(':')[1]).sents)[1])\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1b419e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "engine = pyttsx3.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b782168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially choose a random persona for each chat transcript?\n",
    "voices = engine.getProperty('voices')\n",
    "english_voices = [e for e in voices if any(\n",
    "    'en' in lang for lang in e.languages)]\n",
    "\n",
    "# for voice in english_voices:\n",
    "#     print(\"\\nVoice:\")\n",
    "#     print(\"ID: %s\" % voice.id)\n",
    "#     print(\"Name: %s\" % voice.name)\n",
    "#     print(\"Age: %s\" % voice.age)\n",
    "#     print(\"Gender: %s\" % voice.gender)\n",
    "#     print(\"Languages Known: %s\" % voice.languages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4d89de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.setProperty('rate', 130) # slow it down\n",
    "engine.setProperty('voice', english_voices[5].id) # choose a voice\n",
    "\n",
    "# engine.say(\"i love california its beautiful ! i ' m in my mid 50 ' s . where do you live ?\")\n",
    "# engine.runAndWait()\n",
    "\n",
    "engine.save_to_file(\"i love california its beautiful ! i ' m in my mid 50 ' s . where do you live ?\", 'test.mp3')\n",
    "engine.runAndWait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79247e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "output_dir = Path(\n",
    "    '/Users/samhardyhey/Desktop/otso-rand/experiment_artefacts/stt_data_synthesis/exp_b/doodle_call_a')\n",
    "output_dir.mkdir(\n",
    "    exist_ok=True, parents=True) if output_dir.exists() == False else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, e in enumerate(processed_transcripts):\n",
    "    # format text/save file\n",
    "    text = e.split(': ')[1].lower()\n",
    "#     save_path = output_dir / f\"{idx}_{e.split(': ')[0].lower()}.mp3\"\n",
    "    print(f\"{idx}_{e.split(': ')[0].lower()}.mp3\")\n",
    "    \n",
    "    # synthesize, save locally? cannot save to \"complex\" dir?\n",
    "    f\"{idx}_{e.split(': ')[0].lower()}.mp3\"\n",
    "#     engine.save_to_file(text, f\"{idx}_{e.split(': ')[0].lower()}.mp3\")\n",
    "#     engine.runAndWait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882baed",
   "metadata": {},
   "source": [
    "## Using google speech to sound\n",
    "- https://github.com/pndurette/gTTS\n",
    "- Risk of IP banning? limits unclear https://github.com/pndurette/gTTS/discussions/293\n",
    "- Customizable text pre-processors which can, for example, provide pronunciation corrections\n",
    "- Better audio form, WRT pauses between sentences etc.\n",
    "- Outputs play outside of main application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c9e77a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# basic use\n",
    "import gtts\n",
    "from playsound import playsound\n",
    "\n",
    "text = 'hi i am from the city how are you ? i live in california . where are you from '\n",
    "tts = gtts.gTTS(text, lang='en', tld='com.au', slow=True)\n",
    "tts.save('./hello.mp3')\n",
    "playsound(\"hello.mp3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "37e26a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import time\n",
    "output_dir = Path(\n",
    "    '/Users/samhardyhey/Desktop/otso-rand/experiment_artefacts/stt_data_synthesis/exp_b/doodle_call_a')\n",
    "output_dir.mkdir(\n",
    "    exist_ok=True, parents=True) if output_dir.exists() == False else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "7d5dadae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# in place of pyttsx3\n",
    "for idx, e in enumerate(processed_transcripts):\n",
    "    time.sleep(1) # prevent IP banning?\n",
    "    \n",
    "    # format text/save file\n",
    "    text = e.split(': ')[1].lower()\n",
    "    speaker = int(e.split('_')[1][0])\n",
    "    save_path = output_dir / f\"{idx}_{e.split(': ')[0].lower()}.mp3\"\n",
    "    if speaker == 1:\n",
    "        tts = gtts.gTTS(text, lang='en', tld='com.au', slow=True)\n",
    "    elif speaker == 2:\n",
    "        tts = gtts.gTTS(text, lang='en', tld='co.uk', slow=True)\n",
    "    tts.save(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ccd6bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## How to reconstruct a sequence across multiple channels?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5d77d350",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "22fdec5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# samples = left_channel.get_array_of_samples()\n",
    "# y, s = librosa.load('./hello.mp3') # new downsample rate\n",
    "# soundfile.write(output_dir / f\"{output_dir.name}_{Path(e).name}\", y, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "22a2b11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# pad = np.zeros(y.shape[0], dtype=np.float32)\n",
    "# padded_audio = np.concatenate((pad, y, pad))\n",
    "# soundfile.write('./hello_padded.wav', padded_audio, s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e9a2eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# collate utterance audio files into dataframe\n",
    "audio_file_records = []\n",
    "for e in list(output_dir.rglob('./*.mp3')):\n",
    "    y, s = librosa.load(e.as_posix())  # new downsample rate\n",
    "    audio_file_records.append(\n",
    "        {'file': e.name, 'sample_array': y, 'sample_array_shape': y.shape[0]})\n",
    "\n",
    "df = (pd.DataFrame(audio_file_records)\n",
    "      .sort_values('file')\n",
    "      .assign(channel=lambda x: x.file.apply(lambda y: int(y.split('_')[-1][0])))\n",
    "      .reset_index(drop=True)\n",
    "      )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "e97c6e45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_speaker_1.mp3 is even\n",
      "1_speaker_2.mp3 is odd\n",
      "2_speaker_1.mp3 is even\n",
      "3_speaker_2.mp3 is odd\n",
      "4_speaker_1.mp3 is even\n",
      "5_speaker_2.mp3 is odd\n",
      "6_speaker_1.mp3 is even\n",
      "7_speaker_2.mp3 is odd\n",
      "8_speaker_1.mp3 is even\n",
      "9_speaker_2.mp3 is odd\n"
     ]
    }
   ],
   "source": [
    "# pad channel 1/2 chunks to ensure for interleaving pattern\n",
    "channel_1_segments = []\n",
    "channel_2_segments = []\n",
    "for idx, e in df.iterrows():\n",
    "    if idx % 2 == 0:\n",
    "        print(f\"{e.file} is even\")\n",
    "        # even indices are channel 1; starting first, indexing from zero\n",
    "        channel_1_segments.append(e.sample_array) # the actual samples\n",
    "        # pad alternating channel (channel 2) with equivalent size zero array\n",
    "        channel_2_segments.append(np.zeros(e.sample_array.shape[0], dtype=np.float32))\n",
    "    else:\n",
    "        print(f\"{e.file} is odd\")\n",
    "        # odd indices are channel 2\n",
    "        channel_2_segments.append(e.sample_array)\n",
    "        # otherwise, channel 2 length zero array\n",
    "        channel_1_segments.append(np.zeros(e.sample_array.shape[0], dtype=np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "8434799f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp save for channel 1/2 audio - saves as mono\n",
    "default_sr = 22050\n",
    "channel_1_padded = np.concatenate(channel_1_segments)\n",
    "soundfile.write(output_dir / 'channel_1_temp.wav', channel_1_padded, default_sr)\n",
    "\n",
    "channel_2_padded = np.concatenate(channel_2_segments)\n",
    "soundfile.write(output_dir / 'channel_2_temp.wav', channel_2_padded, default_sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "a310b5d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<_io.BufferedRandom name='/Users/samhardyhey/Desktop/otso-rand/experiment_artefacts/stt_data_synthesis/exp_b/doodle_call_a/consolidated_final.wav'>"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pydub import AudioSegment\n",
    "\n",
    "# consolidate into an interleaving, channel seperated source\n",
    "left_channel = AudioSegment.from_wav(output_dir / 'channel_1_temp.wav')\n",
    "right_channel = AudioSegment.from_wav(output_dir / 'channel_2_temp.wav')\n",
    "\n",
    "# load individual channels...\n",
    "stereo_sound = AudioSegment.from_mono_audiosegments(left_channel, right_channel)\n",
    "stereo_sound.export(output_dir / 'consolidated_final.wav')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
