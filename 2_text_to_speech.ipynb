{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "be7ee38a",
   "metadata": {},
   "source": [
    "## Take a sample ParlAI transcript"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c2edc6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "transcript = \"\"\"[TransformerGenerator_1]: hi i am from the city how are you ? i live in california . where are you from ?\n",
    "   [TransformerGenerator_2]: i ' d love to visit cali . i ' ve never been there . i am 56 .\n",
    "[TransformerGenerator_1]: i love california its beautiful ! i ' m in my mid 50 ' s . where do you live ?\n",
    "   [TransformerGenerator_2]: i live on the west coast . i used too be pretty , but i am not anymore . you ?\n",
    "[TransformerGenerator_1]: i live near san francisco . what do you do for fun ? i am an alcoholic but i love it !\n",
    "   [TransformerGenerator_2]: i love to fantasize . i have dreamed of being the world and taking over . do you ?\n",
    "[TransformerGenerator_1]: i do not ! i love the outdoors . i love being in the mountains ! what are you up to ?\n",
    "   [TransformerGenerator_2]: i was a painter , now i work as a housemaid . you ever been to cali ?\n",
    "[TransformerGenerator_1]: i have not , but my family would love to go . i grew on a very small farm .\n",
    "   [TransformerGenerator_2]: that ' s nice . where did you grow the food ? do you have any hobbies ?\"\"\"\n",
    "\n",
    "processed_transcripts = []\n",
    "for e in transcript.split(\"\\n\"):\n",
    "    # remove brackets, and computer generated names\n",
    "    ee = (\n",
    "        e.strip()\n",
    "        .replace(\"[\", \"\")\n",
    "        .replace(\"]\", \"\")\n",
    "        .replace(\"TransformerGenerator\", \"Speaker\")\n",
    "        .strip()\n",
    "    )\n",
    "    processed_transcripts.append(ee)\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc10eef",
   "metadata": {},
   "source": [
    "## pyttsx3\n",
    "- cannot synthesize a list of strings sequentially? - issue replicated with script, not a notebook issue\n",
    "- cannot supply \"complex\" output dir - eg. nested within dir\n",
    "- can only process list of two strings sequentially\n",
    "- outputed audio files do not play? when generated from script or in sequence\n",
    "- tested a variety of different transcript outputs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26380ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyttsx3==2.90\n",
    "\n",
    "# additionally require libspeak for pyttsx3\n",
    "!apt-get update -y\n",
    "!apt-get install -y libespeak-dev\n",
    "\n",
    "# aplay\n",
    "!apt-get install alsa-utils -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610bb1c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt update && apt install espeak libespeak1 -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2cdfc1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pyttsx3\n",
    "pip list | grep 'pytt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b419e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "engine = pyttsx3.init()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6479b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.save_to_file(\"Hello World\", \"test.mp3\")\n",
    "engine.runAndWait()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2be9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.say(processed_transcripts[0].split(\":\")[1].strip())\n",
    "engine.runAndWait()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b782168e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# potentially choose a random persona for each chat transcript?\n",
    "voices = engine.getProperty(\"voices\")\n",
    "english_voices = [e for e in voices if any(\"en\" in lang for lang in e.languages)]\n",
    "\n",
    "# for voice in english_voices:\n",
    "#     print(\"\\nVoice:\")\n",
    "#     print(\"ID: %s\" % voice.id)\n",
    "#     print(\"Name: %s\" % voice.name)\n",
    "#     print(\"Age: %s\" % voice.age)\n",
    "#     print(\"Gender: %s\" % voice.gender)\n",
    "#     print(\"Languages Known: %s\" % voice.languages)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d89de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.setProperty(\"rate\", 130)  # slow it down\n",
    "engine.setProperty(\"voice\", english_voices[5].id)  # choose a voice\n",
    "\n",
    "# engine.say(\"i love california its beautiful ! i ' m in my mid 50 ' s . where do you live ?\")\n",
    "# engine.runAndWait()\n",
    "\n",
    "engine.save_to_file(\n",
    "    \"i love california its beautiful ! i ' m in my mid 50 ' s . where do you live ?\",\n",
    "    \"test.mp3\",\n",
    ")\n",
    "engine.runAndWait()\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79247e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "output_dir = Path(\"/path/to/call\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True) if output_dir.exists() == False else None\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205c5219",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, e in enumerate(processed_transcripts):\n",
    "    # format text/save file\n",
    "    text = e.split(\": \")[1].lower()\n",
    "    #     save_path = output_dir / f\"{idx}_{e.split(': ')[0].lower()}.mp3\"\n",
    "    print(f\"{idx}_{e.split(': ')[0].lower()}.mp3\")\n",
    "\n",
    "    # synthesize, save locally? cannot save to \"complex\" dir?\n",
    "    f\"{idx}_{e.split(': ')[0].lower()}.mp3\"\n",
    "#     engine.save_to_file(text, f\"{idx}_{e.split(': ')[0].lower()}.mp3\")\n",
    "#     engine.runAndWait()\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4882baed",
   "metadata": {},
   "source": [
    "## Google TTS\n",
    "- https://github.com/pndurette/gTTS\n",
    "- Risk of IP banning? limits unclear https://github.com/pndurette/gTTS/discussions/293\n",
    "- Customizable text pre-processors which can, for example, provide pronunciation corrections\n",
    "- Better audio form, WRT pauses between sentences etc.\n",
    "- Outputs play outside of main application!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e77a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gtts\n",
    "import IPython\n",
    "\n",
    "tts = gtts.gTTS(\n",
    "    processed_transcripts[0].split(\":\")[1].strip(), lang=\"en\", tld=\"com.au\", slow=True\n",
    ")\n",
    "tts.save(\"./output/synth_calls/gtts_sample.mp3\")\n",
    "IPython.display.Audio(\"./output/synth_calls/sample_transcript/consolidated_final.wav\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f6964cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "IPython.display.Audio(\"./output/synth_calls/sample_transcript/consolidated_final.wav\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64de5eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import gtts\n",
    "\n",
    "# store temp TTS utterance audio\n",
    "output_dir = Path(\"./output/synth_calls/sample_transcript\")\n",
    "output_dir.mkdir(exist_ok=True, parents=True) if output_dir.exists() == False else None\n",
    "\n",
    "# TTS for each utterance\n",
    "for idx, utterance in enumerate(transcript.split(\"\\n\")):\n",
    "    time.sleep(1)  # ip ban?\n",
    "    speaker = \"speaker_1\" if idx % 2 == 0 else \"speaker_2\"\n",
    "    formatted_utterance = utterance.split(\":\")[1].strip()\n",
    "    output_file = output_dir / f\"{speaker}_utterance_{idx}.mp3\"\n",
    "    tts = gtts.gTTS(formatted_utterance, lang=\"en\", tld=\"com.au\", slow=True)\n",
    "    tts.save(output_file)\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c909d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pydub\n",
    "!pip list | grep 'pydub'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e26a55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import pandas as pd\n",
    "from pydub import AudioSegment\n",
    "\n",
    "audio_fragment_records = []\n",
    "for file in list(output_dir.glob(\"./*.mp3\")):\n",
    "    # collate utterance audio files into raw samples\n",
    "    y, s = librosa.load(str(file))  # FYI: assigns default sample rate\n",
    "    audio_fragment_records.append(\n",
    "        {\"file\": file.name, \"sample_array\": y, \"sample_array_shape\": y.shape[0]}\n",
    "    )\n",
    "\n",
    "audio_fragments = (\n",
    "    pd.DataFrame(audio_fragment_records)\n",
    "    # probably just the df index; but to be sure\n",
    "    .assign(sequence_idx=lambda x: x.file.apply(lambda y: int(y.split(\"_\")[-1][0])))\n",
    "    .sort_values(\"sequence_idx\")\n",
    "    # speaker as channel\n",
    "    .assign(channel=lambda x: x.sequence_idx.apply(lambda y: 1 if y % 2 == 0 else 2))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03709992",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# pad channel 1/2 chunks to ensure for interleaving pattern\n",
    "channel_1_segments = []\n",
    "channel_2_segments = []\n",
    "for idx, e in audio_fragments.iterrows():\n",
    "\n",
    "    if e.channel == 1:\n",
    "        channel_1_segments.append(e.sample_array)\n",
    "        # pad alternating channel (channel 2) with equivalent size zero array to create interleave\n",
    "        channel_2_segments.append(np.zeros(e.sample_array.shape[0], dtype=np.float32))\n",
    "    else:\n",
    "        # odd indices are channel 2\n",
    "        channel_2_segments.append(e.sample_array)\n",
    "        # otherwise, channel 2 length zero array\n",
    "        channel_1_segments.append(np.zeros(e.sample_array.shape[0], dtype=np.float32))\n",
    "\n",
    "# temp save for channel 1/2 audio - saves as mono\n",
    "default_sr = 22050\n",
    "channel_1_padded = np.concatenate(channel_1_segments)\n",
    "soundfile.write(output_dir / \"channel_1_temp.wav\", channel_1_padded, default_sr)\n",
    "\n",
    "channel_2_padded = np.concatenate(channel_2_segments)\n",
    "soundfile.write(output_dir / \"channel_2_temp.wav\", channel_2_padded, default_sr)\n",
    "\n",
    "# consolidate into an interleaving, channel seperated source\n",
    "left_channel = AudioSegment.from_wav(output_dir / \"channel_1_temp.wav\")\n",
    "right_channel = AudioSegment.from_wav(output_dir / \"channel_2_temp.wav\")\n",
    "\n",
    "stereo_sound = AudioSegment.from_mono_audiosegments(left_channel, right_channel)\n",
    "stereo_sound.export(output_dir / \"consolidated_final.wav\")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
