{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93f9a5e",
   "metadata": {},
   "source": [
    "## Samsum dataset\n",
    "- Easy access\n",
    "- Decent quality\n",
    "- Some commercial limitations noted: https://huggingface.co/datasets/samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe18b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py7zr\n",
    "!pip list | grep 'py7zr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101830e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")\n",
    "samsum = pd.DataFrame(dataset[\"test\"])\n",
    "samsum.iloc[0].dialogue.split(\"\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d2a19",
   "metadata": {},
   "source": [
    "## GPT generative conversation\n",
    "- Using microsoft/DialoGPT-medium and large\n",
    "- Pytorch weight loading issues noted with blenderbot large/small variants: https://huggingface.co/transformers/model_doc/blenderbot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96013a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:07:31.472113Z",
     "start_time": "2021-08-25T03:59:48.337290Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89ddea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:28:54.894564Z",
     "start_time": "2021-08-25T04:27:48.288366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(\n",
    "        input(\">> User:\") + tokenizer.eos_token, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = (\n",
    "        torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "        if step > 0\n",
    "        else new_user_input_ids\n",
    "    )\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens,\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\n",
    "        \"DialoGPT: {}\".format(\n",
    "            tokenizer.decode(\n",
    "                chat_history_ids[:, bot_input_ids.shape[-1] :][0],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc84073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:48:54.335982Z",
     "start_time": "2021-08-25T04:48:45.094578Z"
    }
   },
   "source": [
    "from transformers import BlenderbotSmallTokenizer, BlenderbotSmallForConditionalGeneration\n",
    "\n",
    "mname = 'facebook/blenderbot_small-90M'\n",
    "model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotSmallTokenizer.from_pretrained(mname)\n",
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "print(\"Human: \", UTTERANCE)\n",
    "inputs = tokenizer([UTTERANCE], return_tensors='pt')\n",
    "\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "# what kind of carbs do they eat? i don't know much about carbs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991c8b7",
   "metadata": {},
   "source": [
    "## ParlAI\n",
    "- Options for notebooks/source as well as CLI scripts configured across a variety of tasks\n",
    "- Able to create CLI chat interface, with human in the loop (chit-chat task)\n",
    "- Able to loop model back onto itself (model-model), useful for programmatically generating large conversational datasets?\n",
    "- Grammar/sentence formation is an issue, though this is not an issue in terms of down-stream TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95511e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive prompts\n",
    "parlai interactive --model-file zoo:blender/blender_90M/model --task convai2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa1b8d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tasks - \n",
    "# models - zoo:blender/blender_90M/model\n",
    "\n",
    "parlai self_chat --model-file zoo:blender/blender_90M/model --task convai2 --inference topk --num-self-chats 1 --display-examples True --datatype valid --outfile 'self_chat_test.jsonl'\n",
    "parlai self_chat --model-file zoo:blender/blender_400Mdistill/model --task blended_skill_talk --inference topk --num-self-chats 1 --display-examples True --datatype valid --outfile 'self_chat_test.jsonl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9434206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self chat\n",
    "parlai self_chat --model-file zoo:blender/blender_90M/model --task convai2 --inference topk --num-self-chats 10 --display-examples True --datatype valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed with below conversation_starters text file\n",
    "/home/samhardyhey/.local/bin/parlai self_chat --model-file zoo:blender/blender_90M/model --task convai2 --inference topk --num-self-chats 2 --selfchat-max-turns 6 --display-examples True --datatype valid --seed-messages-from-file /home/samhardyhey/conversation_starters.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4caec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:11:07.040306Z",
     "start_time": "2021-08-25T07:11:06.882154Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /home/samhardyhey/conversation_starters.txt\n",
    "Ponzi scheme insurer\n",
    "Time wasters and SUPER expensive\n",
    "Bramdon was an excellent customer service provider rather than an agent."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dc0691a",
   "metadata": {},
   "source": [
    "## Invoke ParlAI via script\n",
    "- inclusion within final synthesize_transcripts.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a4f3488",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "# model\n",
    "# task\n",
    "# num self chats\n",
    "# outfile\n",
    "\n",
    "subprocess.call(['parlai','self_chat','--model-file','zoo:blender/blender_90M/model', '--task','convai2','--inference','topk','--num-self-chats','10','--display-examples','True','--datatype','valid', '--outfile','self_chat_test.jsonl'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1556e93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import srsly\n",
    "import pandas as pd\n",
    "import shortuuid\n",
    "\n",
    "# list(srsly.read_jsonl('./self_chat_test.jsonl'))[0]\n",
    "# list(output_dir.glob(\"./*.mp3\"))\n",
    "\n",
    "def format_episode(episode_raw):\n",
    "    # as a dataframe\n",
    "    episode = []\n",
    "    for e in episode_raw[\"dialog\"]:\n",
    "        episode.extend([{\"id\": ee[\"id\"], \"text\": ee[\"text\"]} for ee in e])\n",
    "\n",
    "    return (\n",
    "        pd.DataFrame(episode)\n",
    "        .reset_index()\n",
    "        .rename(mapper={\"index\": \"exchange_index\"}, axis=\"columns\")\n",
    "        .assign(speaker=lambda x: x.id.apply(lambda y: int(y.split(\"_\")[1])))\n",
    "        .assign(episode_id=lambda x: [shortuuid.uuid() for e in range(x.shape[0])])\n",
    "    )\n",
    "\n",
    "\n",
    "def synthesize_tts_episode(episode_df, output_dir):\n",
    "    # given an episode DF, synthesize audio for each utterance\n",
    "    for idx, e in episode_df.iterrows():\n",
    "        time.sleep(1)  # prevent IP banning?\n",
    "        # format text/save file\n",
    "        save_path = output_dir / f\"{e.exchange_index}_speaker_{e.speaker}.mp3\"\n",
    "\n",
    "        # alternative voices, useful for debugging, could be improved with more variance\n",
    "        if e.speaker == 1:\n",
    "            tts = gtts.gTTS(e.text, lang=\"en\", tld=\"com\", slow=True)\n",
    "        elif e.speaker == 2:\n",
    "            tts = gtts.gTTS(e.text, lang=\"en\", tld=\"ca\", slow=True)\n",
    "\n",
    "        tts.save(save_path)\n",
    "\n",
    "# base_output_dir = Path(\"/path/to/synthesis\")\n",
    "# # a collection of self-chat episodes\n",
    "self_chat = list(srsly.read_jsonl(\"./self_chat_test.jsonl\"))\n",
    "\n",
    "# for episode in self_chat:\n",
    "episode = self_chat[0]\n",
    "# format episode\n",
    "episode = format_episode(episode).head(8)\n",
    "\n",
    "# # create unique output dir\n",
    "# episode_id = uuid4().hex\n",
    "# output_dir = base_output_dir / episode_id\n",
    "# output_dir.mkdir(exist_ok=True, parents=True) if output_dir.exists() == False else None\n",
    "\n",
    "# # synthesize, save audio\n",
    "# synthesize_tts_episode(episode, output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25257c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install shortuuid\n",
    "!pip list | grep 'shortuuid'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
