{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "694fc2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# requirements\n",
    "transformers==4.21.0\n",
    "datasets==2.4.0\n",
    "torch==1.10.0\n",
    "pandas==1.4.3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d93f9a5e",
   "metadata": {},
   "source": [
    "## Samsum dataset\n",
    "- Can access just fine\n",
    "- Some commercial limitations though: https://huggingface.co/datasets/samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21f7193f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T22:51:37.020191Z",
     "start_time": "2021-08-25T22:51:35.936968Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/samhardy/Desktop/blog/blog.venv/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Reusing dataset samsum (/Users/samhardy/.cache/huggingface/datasets/samsum/samsum/0.0.0/f1d7c6b7353e6de335d444e424dc002ef70d1277109031327bc9cc6af5d3d46e)\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:00<00:00, 1102.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")\n",
    "samsum = pd.DataFrame(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "478ba2e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T22:51:40.294151Z",
     "start_time": "2021-08-25T22:51:40.281098Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"Hannah: Hey, do you have Betty's number?\",\n",
       " 'Amanda: Lemme check',\n",
       " 'Hannah: <file_gif>',\n",
       " \"Amanda: Sorry, can't find it.\",\n",
       " 'Amanda: Ask Larry',\n",
       " 'Amanda: He called her last time we were at the park together',\n",
       " \"Hannah: I don't know him well\",\n",
       " 'Hannah: <file_gif>',\n",
       " \"Amanda: Don't be shy, he's very nice\",\n",
       " 'Hannah: If you say so..',\n",
       " \"Hannah: I'd rather you texted him\",\n",
       " 'Amanda: Just text him ðŸ™‚',\n",
       " 'Hannah: Urgh.. Alright',\n",
       " 'Hannah: Bye',\n",
       " 'Amanda: Bye bye']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "samsum.iloc[0].dialogue.split('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d2a19",
   "metadata": {},
   "source": [
    "## GPT generative conversation\n",
    "- Using microsoft/DialoGPT-medium and large\n",
    "- Pytorch weight loading issues noted with blenderbot large/small variants: https://huggingface.co/transformers/model_doc/blenderbot.html\n",
    "- EleutherAI GPT neo is ~10GB download or so"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f75d90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T03:38:56.925860Z",
     "start_time": "2021-08-25T03:38:09.851365Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "generator = pipeline('text-generation', model='EleutherAI/gpt-neo-2.7B')\n",
    "generator(\"EleutherAI has\", do_sample=True, min_length=50)\n",
    "\n",
    "# [{'generated_text': 'EleutherAI has made a commitment to create new software packages for each of its major clients and has'}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96013a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:07:31.472113Z",
     "start_time": "2021-08-25T03:59:48.337290Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89ddea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:28:54.894564Z",
     "start_time": "2021-08-25T04:27:48.288366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(input(\">> User:\") + tokenizer.eos_token, return_tensors='pt')\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = torch.cat([chat_history_ids, new_user_input_ids], dim=-1) if step > 0 else new_user_input_ids\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens, \n",
    "    chat_history_ids = model.generate(bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id)\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc84073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:48:54.335982Z",
     "start_time": "2021-08-25T04:48:45.094578Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import BlenderbotSmallTokenizer, BlenderbotSmallForConditionalGeneration\n",
    "\n",
    "mname = 'facebook/blenderbot_small-90M'\n",
    "model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotSmallTokenizer.from_pretrained(mname)\n",
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "print(\"Human: \", UTTERANCE)\n",
    "inputs = tokenizer([UTTERANCE], return_tensors='pt')\n",
    "inputs.pop(\"token_type_ids\")\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "# what kind of carbs do they eat? i don't know much about carbs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991c8b7",
   "metadata": {},
   "source": [
    "## ParlAI\n",
    "- Options for notebooks/source as well as CLI scripts configured across a variety of tasks\n",
    "- Able to create CLI chat interface, with human in the loop (chit-chat task)\n",
    "- Able to loop model back onto itself (model-model), useful for programmatically generating large conversational datasets?\n",
    "- Grammar/sentence formation is an issue, though this is not an issue in terms of down-stream TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95511e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive prompts\n",
    "/home/samhardyhey/.local/bin/parlai interactive --model-file zoo:blender/blender_90M/model --task convai2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9434206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self chat\n",
    "/home/samhardyhey/.local/bin/parlai self_chat --model-file zoo:blender/blender_90M/model --task convai2 --inference topk --num-self-chats 10 --display-examples True --datatype valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed with below conversation_starters text file\n",
    "/home/samhardyhey/.local/bin/parlai self_chat --model-file zoo:blender/blender_90M/model --task convai2 --inference topk --num-self-chats 2 --selfchat-max-turns 6 --display-examples True --datatype valid --seed-messages-from-file /home/samhardyhey/conversation_starters.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d4caec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:11:07.040306Z",
     "start_time": "2021-08-25T07:11:06.882154Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting /home/samhardyhey/conversation_starters.txt\n"
     ]
    }
   ],
   "source": [
    "%%writefile /home/samhardyhey/conversation_starters.txt\n",
    "Ponzi scheme insurer\n",
    "Time wasters and SUPER expensive\n",
    "Bramdon was an excellent customer service provider rather than an agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
