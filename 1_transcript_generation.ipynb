{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d93f9a5e",
   "metadata": {},
   "source": [
    "## Samsum dataset\n",
    "- Easy access\n",
    "- Decent quality\n",
    "- Some commercial limitations noted: https://huggingface.co/datasets/samsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe18b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install py7zr\n",
    "!pip list | grep 'py7zr'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f7193f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T22:51:37.020191Z",
     "start_time": "2021-08-25T22:51:35.936968Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset = load_dataset(\"samsum\")\n",
    "samsum = pd.DataFrame(dataset[\"test\"])\n",
    "samsum.iloc[0].dialogue.split(\"\\n\")\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b51d2a19",
   "metadata": {},
   "source": [
    "## GPT generative conversation\n",
    "- Using microsoft/DialoGPT-medium and large\n",
    "- Pytorch weight loading issues noted with blenderbot large/small variants: https://huggingface.co/transformers/model_doc/blenderbot.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a96013a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:07:31.472113Z",
     "start_time": "2021-08-25T03:59:48.337290Z"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/DialoGPT-large\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb89ddea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:28:54.894564Z",
     "start_time": "2021-08-25T04:27:48.288366Z"
    }
   },
   "outputs": [],
   "source": [
    "# Let's chat for 5 lines\n",
    "for step in range(5):\n",
    "    # encode the new user input, add the eos_token and return a tensor in Pytorch\n",
    "    new_user_input_ids = tokenizer.encode(\n",
    "        input(\">> User:\") + tokenizer.eos_token, return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    # append the new user input tokens to the chat history\n",
    "    bot_input_ids = (\n",
    "        torch.cat([chat_history_ids, new_user_input_ids], dim=-1)\n",
    "        if step > 0\n",
    "        else new_user_input_ids\n",
    "    )\n",
    "\n",
    "    # generated a response while limiting the total chat history to 1000 tokens,\n",
    "    chat_history_ids = model.generate(\n",
    "        bot_input_ids, max_length=1000, pad_token_id=tokenizer.eos_token_id\n",
    "    )\n",
    "\n",
    "    # pretty print last ouput tokens from bot\n",
    "    print(\n",
    "        \"DialoGPT: {}\".format(\n",
    "            tokenizer.decode(\n",
    "                chat_history_ids[:, bot_input_ids.shape[-1] :][0],\n",
    "                skip_special_tokens=True,\n",
    "            )\n",
    "        )\n",
    "    )\n",
    ""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc84073",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T04:48:54.335982Z",
     "start_time": "2021-08-25T04:48:45.094578Z"
    }
   },
   "source": [
    "from transformers import BlenderbotSmallTokenizer, BlenderbotSmallForConditionalGeneration\n",
    "\n",
    "mname = 'facebook/blenderbot_small-90M'\n",
    "model = BlenderbotSmallForConditionalGeneration.from_pretrained(mname)\n",
    "tokenizer = BlenderbotSmallTokenizer.from_pretrained(mname)\n",
    "UTTERANCE = \"My friends are cool but they eat too many carbs.\"\n",
    "print(\"Human: \", UTTERANCE)\n",
    "inputs = tokenizer([UTTERANCE], return_tensors='pt')\n",
    "\n",
    "reply_ids = model.generate(**inputs)\n",
    "print(\"Bot: \", tokenizer.batch_decode(reply_ids, skip_special_tokens=True)[0])\n",
    "# what kind of carbs do they eat? i don't know much about carbs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a991c8b7",
   "metadata": {},
   "source": [
    "## ParlAI\n",
    "- Options for notebooks/source as well as CLI scripts configured across a variety of tasks\n",
    "- Able to create CLI chat interface, with human in the loop (chit-chat task)\n",
    "- Able to loop model back onto itself (model-model), useful for programmatically generating large conversational datasets?\n",
    "- Grammar/sentence formation is an issue, though this is not an issue in terms of down-stream TTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4874747c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install parlai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95511e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# interactive prompts\n",
    "parlai interactive --model-file zoo:blender/blender_90M/model --task convai2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9434206",
   "metadata": {},
   "outputs": [],
   "source": [
    "# self chat\n",
    "parlai self_chat --model-file zoo:blender/blender_90M/model --task convai2 --inference topk --num-self-chats 10 --display-examples True --datatype valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e1b661c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# seed with below conversation_starters text file\n",
    "/home/samhardyhey/.local/bin/parlai self_chat --model-file zoo:blender/blender_90M/model --task convai2 --inference topk --num-self-chats 2 --selfchat-max-turns 6 --display-examples True --datatype valid --seed-messages-from-file /home/samhardyhey/conversation_starters.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4caec0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-08-25T07:11:07.040306Z",
     "start_time": "2021-08-25T07:11:06.882154Z"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile /home/samhardyhey/conversation_starters.txt\n",
    "Ponzi scheme insurer\n",
    "Time wasters and SUPER expensive\n",
    "Bramdon was an excellent customer service provider rather than an agent."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
