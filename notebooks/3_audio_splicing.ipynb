{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9f0d3d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-09-23T02:16:31.483621Z",
     "start_time": "2021-09-23T02:16:31.479843Z"
    }
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import gtts\n",
    "import librosa\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import soundfile\n",
    "import srsly\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "from pydub import AudioSegment\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "pd.options.display.max_colwidth = 100\n",
    "\n",
    "\n",
    "output_dir = Path(\"./output/synth_calls/sample_transcript\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da266e88",
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_fragment_records = []\n",
    "for file in list(output_dir.glob(\"./*.mp3\")):\n",
    "    # collate utterance audio files into raw samples\n",
    "    y, s = librosa.load(str(file))  # FYI: assigns default sample rate\n",
    "    audio_fragment_records.append(\n",
    "        {\"file\": file.name, \"sample_array\": y, \"sample_array_shape\": y.shape[0]}\n",
    "    )\n",
    "\n",
    "audio_fragments = (\n",
    "    pd.DataFrame(audio_fragment_records)\n",
    "    # probably just the df index; but to be sure\n",
    "    .assign(sequence_idx=lambda x: x.file.apply(lambda y: int(y.split(\"_\")[-1][0])))\n",
    "    .sort_values(\"sequence_idx\")\n",
    "    # speaker as channel\n",
    "    .assign(channel=lambda x: x.sequence_idx.apply(lambda y: 1 if y % 2 == 0 else 2))\n",
    "    .reset_index(drop=True)\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a030121",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import soundfile\n",
    "from pydub import AudioSegment\n",
    "\n",
    "# pad channel 1/2 chunks to ensure for interleaving pattern\n",
    "channel_1_segments = []\n",
    "channel_2_segments = []\n",
    "for idx, e in audio_fragments.iterrows():\n",
    "\n",
    "    if e.channel == 1:\n",
    "        channel_1_segments.append(e.sample_array)\n",
    "        # pad alternating channel (channel 2) with equivalent size zero array to create interleave\n",
    "        channel_2_segments.append(np.zeros(e.sample_array.shape[0], dtype=np.float32))\n",
    "    else:\n",
    "        # odd indices are channel 2\n",
    "        channel_2_segments.append(e.sample_array)\n",
    "        # otherwise, channel 2 length zero array\n",
    "        channel_1_segments.append(np.zeros(e.sample_array.shape[0], dtype=np.float32))\n",
    "\n",
    "# temp save for channel 1/2 audio - saves as mono\n",
    "default_sr = 22050\n",
    "channel_1_padded = np.concatenate(channel_1_segments)\n",
    "soundfile.write(output_dir / \"channel_1_temp.wav\", channel_1_padded, default_sr)\n",
    "\n",
    "channel_2_padded = np.concatenate(channel_2_segments)\n",
    "soundfile.write(output_dir / \"channel_2_temp.wav\", channel_2_padded, default_sr)\n",
    "\n",
    "# consolidate into an interleaving, channel seperated source\n",
    "left_channel = AudioSegment.from_wav(output_dir / \"channel_1_temp.wav\")\n",
    "right_channel = AudioSegment.from_wav(output_dir / \"channel_2_temp.wav\")\n",
    "\n",
    "stereo_sound = AudioSegment.from_mono_audiosegments(left_channel, right_channel)\n",
    "stereo_sound.export(output_dir / \"consolidated_final.wav\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56b82607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # put the final wavs in a single dir\n",
    "# episode_wav_dir = base_output_dir / \"episode_wavs\"\n",
    "# episode_wav_dir.mkdir(\n",
    "#     exist_ok=True, parents=True\n",
    "# ) if episode_wav_dir.exists() == False else None\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2477c03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for e in list(output_dir.parents[0].rglob(\"./*.wav\")):\n",
    "    if \"final\" in e.as_posix():\n",
    "        shutil.move(str(e), output_dir.parents[1] / f\"final_calls/{e.name}.wav\")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b78c3a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean up? nah, files are pretty small > git repo\n",
    "import shutil\n",
    "\n",
    "[e.unlink() for e in output_dir.glob(\"./*.mp3\")]\n",
    "[e.unlink() for e in output_dir.glob(\"./*.wav\") if \"temp\" in str(e)]\n",
    "\n",
    "shutil.move(\n",
    "    str(output_dir / \"consolidated_final.wav\"),\n",
    "    output_dir.parents[1] / f\"final_calls/{output_dir.name}.wav\",\n",
    ")\n",
    ""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7442c5ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython\n",
    "\n",
    "IPython.display.Audio(\n",
    "    \"/home/asr-synthesis-blog/output/final_calls/mDHvtCPoQGeebm5oCSXXUa.wav\"\n",
    ")\n",
    ""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('blog.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "b352da5c727154a09156c935f17a9c4d49b2c9c0946f47ddfcca219f38b45087"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
